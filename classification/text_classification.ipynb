{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 自然语言处理入门\n",
    "\n",
    "自然语言处理(NLP, Natural Language Process)\n",
    "\n",
    "处理自然语言的python库：nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 分词\n",
    "\n",
    "分词(Tokenization): 将文本划分为词语的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’ve spent the last 14 years in sales and business development for investment research firms and capital market data vendors. I am now on a journey to pivot my career from data sales to data analytics. I am learning a lot of cool stuff about artificial intelligence, machine learning and how to use Python code to make use of these techniques for investing. I have found that the fastest way to learn something new is to try to teach it. So I’d like to share what I’m learning as I go along.\n"
     ]
    }
   ],
   "source": [
    "text = \"I’ve spent the last 14 years in sales and business development for investment research firms and capital market data vendors. I am now on a journey to pivot my career from data sales to data analytics. I am learning a lot of cool stuff about artificial intelligence, machine learning and how to use Python code to make use of these techniques for investing. I have found that the fastest way to learn something new is to try to teach it. So I’d like to share what I’m learning as I go along.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_tokenize()方法能够将标准词汇和标点符号分开。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '’', 've', 'spent', 'the', 'last', '14', 'years', 'in', 'sales', 'and', 'business', 'development', 'for', 'investment', 'research', 'firms', 'and', 'capital', 'market', 'data', 'vendors', '.', 'I', 'am', 'now', 'on', 'a', 'journey', 'to', 'pivot', 'my', 'career', 'from', 'data', 'sales', 'to', 'data', 'analytics', '.', 'I', 'am', 'learning', 'a', 'lot', 'of', 'cool', 'stuff', 'about', 'artificial', 'intelligence', ',', 'machine', 'learning', 'and', 'how', 'to', 'use', 'Python', 'code', 'to', 'make', 'use', 'of', 'these', 'techniques', 'for', 'investing', '.', 'I', 'have', 'found', 'that', 'the', 'fastest', 'way', 'to', 'learn', 'something', 'new', 'is', 'to', 'try', 'to', 'teach', 'it', '.', 'So', 'I', '’', 'd', 'like', 'to', 'share', 'what', 'I', '’', 'm', 'learning', 'as', 'I', 'go', 'along', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk_tokens = nltk.word_tokenize(text)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 提取词干\n",
    "\n",
    "词干(stemming): 词语背后的核心概念。\n",
    "\n",
    "很多单词代表的概念是相同的，例如'am','is','be','are'都代表'是'，'tables'和'table'都代表桌子。\n",
    "\n",
    "**为什么要提取词干？**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> i\n",
      "’ --> ’\n",
      "ve --> ve\n",
      "spent --> spent\n",
      "the --> the\n",
      "last --> last\n",
      "14 --> 14\n",
      "years --> year\n",
      "in --> in\n",
      "sales --> sal\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "for token in nltk_tokens[:10]:\n",
    "    token_stem = stemmer.stem(token)\n",
    "    print(f\"{token} --> {token_stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 词性标注\n",
    "\n",
    "词性标注(POS-tagging, Part Of Speech tagging): 标注词语的性质，性质包括名词，动词，连词等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP\n",
      "’ VBP\n",
      "ve JJ\n",
      "spent VBD\n",
      "the DT\n",
      "last JJ\n",
      "14 CD\n",
      "years NNS\n",
      "in IN\n",
      "sales NNS\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(nltk_tokens)\n",
    "for token, tag in tags[:10]:\n",
    "    print(token, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 停止词\n",
    "\n",
    "停止词(stopword): 使用频率很高但含义很少的词语，例如'the', 'this', 'as'。\n",
    "\n",
    "在特征选择阶段，通常要剔除停止词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 文本分类\n",
    "\n",
    "采用newsgroups数据集，建立简单的文本分类模型，预测新闻文本属于哪个类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 newsgroups数据集\n",
    "\n",
    "\"新闻组(newsgroups)\"专门用于训练文本分类模型。数据集总共包含18000多条新闻，分属于20个类别，已经被划分为训练集和检验集。\n",
    "\n",
    "sklearn.datasets.fetch_20newsgroups接口用于下载/从本地加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从本地加载数据，若本地无数据会自动下载\n",
    "newsgroups = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data属性包含了所有的新闻文本\n",
    "print(newsgroups.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 所有新闻被划分为20个不同的类别\n",
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 多项式朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8016759776536313"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为简单起见，加载几个类别的数据\n",
    "categories = [\n",
    "    'talk.religion.misc',\n",
    "    'soc.religion.christian',\n",
    "    'sci.space',\n",
    "    'comp.graphics'\n",
    "]\n",
    "train = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
    "test = fetch_20newsgroups(subset=\"test\", categories=categories)\n",
    "\n",
    "# 处理文本数据，用Tfidf法将文本转化为数值矩阵\n",
    "# 第一次调用fit_transform后已经获取所有文档的单词量，后续只需要调用transform即可\n",
    "vec = TfidfVectorizer()\n",
    "X_train = vec.fit_transform(train.data)\n",
    "X_test = vec.transform(test.data)\n",
    "y_train = train.target\n",
    "y_test = test.target\n",
    "\n",
    "# 多项式朴素贝叶斯分类器\n",
    "model = MultinomialNB()\n",
    "\n",
    "# 拟合数据，训练模型\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 用预测精度简单评估分类器的性能\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 模型优化\n",
    "\n",
    "先对原始文本进行加工处理，如词语分词，提炼词干等，看是否能提供分类器性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\": revdak @ netcom.com ( d. andrew kil ) subject : : serb genocid work god ? org : netcom on-line commun serv ( 408 241-9760 guest ) lin : 22 jam sled ( jsledd @ ssdc.sas.upenn.edu ) wrot : : serb work god ? hmm ... : 've wond anyon would ev ask quest , : govern unit stat europ mov : end ethn cleans serb target : muslim ? : can/does god us follow accompl : task ? esp task pun ? : jam sled : cut sig ... . 'm work . suggest god support genocid ? perhap germ `` pun '' jew god 's behalf ? god work way indescrib evil , unworthy wor fai . revdak @ netcom.com\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # 词语分词\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    # 剔除停止词\n",
    "    tokens_ex_stopwords = [token for token in tokens if token not in stop_words]\n",
    "    # 提炼词干\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens_ex_stopwords]\n",
    "    return \" \".join(tokens_stem)\n",
    "    \n",
    "clean_text(train.data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8400837988826816"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先进行文本预处理\n",
    "train_clean = [clean_text(text) for text in train.data]\n",
    "test_clean = [clean_text(text) for text in test.data]\n",
    "\n",
    "# 用Tfidf法将文本转化为数值矩阵\n",
    "# 第一次调用fit_transform后已经获取所有文档的单词量，后续只需要调用transform即可\n",
    "vec = TfidfVectorizer()\n",
    "X_train = vec.fit_transform(train_clean)\n",
    "X_test = vec.transform(test_clean)\n",
    "y_train = train.target\n",
    "y_test = test.target\n",
    "\n",
    "# 多项式朴素贝叶斯分类器\n",
    "model = MultinomialNB()\n",
    "\n",
    "# 拟合数据，训练模型\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 用预测精度简单评估分类器的性能\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行预处理后预测精度提升了近4个百分点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
